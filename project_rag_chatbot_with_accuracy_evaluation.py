# -*- coding: utf-8 -*-
"""PROJECT - RAG CHATBOT WITH ACCURACY EVALUATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H_HdQhD2s-_fZzoUitFza2V5nCN47vV9

# **PROJECT - RAG CHATBOT WITH ACCURACY EVALUATION**

## **Project Description**
**This project aims to build a Retrieval-Augmented Generation (RAG) chatbot that answers questions based on content extracted from a given PDF. The chatbot uses FAISS for similarity search and a Large Language Model (LLM) for response generation. Additionally, it evaluates the accuracy of its answers using ROUGE-1 F1 Score, comparing generated responses with expected answers from the PDF.**

## **Dataset: (PDF Document)**
**The chatbot retrieves answers from DSUnit1.pdf, which contains structured information relevant to user queries. FAISS is used to vectorize and search for the most relevant document chunks.**

## **Technologies Used:**
**FAISS â€“ For efficient document similarity search**
**Transformers (Hugging Face) â€“ For LLM-based text generation**
**Gradio â€“ To create an interactive chatbot UI**
**ROUGE Score â€“ For evaluating response accuracy**

## **Model & Approach:**
**PDF Processing: Extracts text from DSUnit1.pdf.**
**Vector Search (FAISS): Retrieves the most relevant document chunk for a given query.**
**LLM Response Generation: Uses LLaMA-2-7B to generate answers.**
**Accuracy Evaluation: Compares generated responses with correct answers using ROUGE-1 F1 Score.**

## **Evaluation Metric:**
**ROUGE-1 F1 Score: Measures the overlap of words between the chatbotâ€™s response and the correct answer extracted from the PDF.**

## **Expected Outcome:**
**A chatbot capable of answering PDF-based queries with high accuracy.**
**An accuracy score that helps determine the effectiveness of the chatbotâ€™s responses.**
**A user-friendly interface where users can input queries and receive structured answers.**

# **INSTALLING DEPENDENCIES:**
"""

!pip install transformers
!pip install sentence-transformers
!pip install PyPDF2
!pip install faiss-cpu
!pip install langchain chromadb
!pip install pypdf
!pip install torch
!pip install -U langchain-community
!pip install -U langchain

"""# **IMPORTING ESSENTIAL LIBRARIES:**"""

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM
import torch
import os
!pip install Gradio
import gradio as gr

"""# **LOADING AND SPLITTING THE PDF:**"""

pdf_files = [f for f in os.listdir() if f.endswith('.pdf')]
if not pdf_files:
    raise FileNotFoundError("No PDF files found! Please upload a PDF to Colab.")
pdf_path = pdf_files[0]
print(f"Using PDF: {pdf_path}")

loader = PyPDFLoader(pdf_path)
documents = loader.load()
print(f"Loaded {len(documents)} pages.")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)
print(f"Split into {len(chunks)} chunks.")
print(chunks[0].page_content[:500])

"""# **CREATING EMBEDDINGS FOR SEARCH:**"""

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
chunk_embeddings = embeddings.embed_documents([chunk.page_content for chunk in chunks])
print(f"Created embeddings for {len(chunk_embeddings)} chunks.")
print(f"Embedding size: {len(chunk_embeddings[0])} dimensions.")

"""# **STORING EMBEDDINGS IN FAISS:**"""

vector_store = FAISS.from_documents(chunks, embeddings)
print(f"Stored {vector_store.index.ntotal} embeddings in FAISS.")

# Optional: Test the setup with a sample query
query = "What is a protocol stack?"
similar_docs = vector_store.similarity_search(query, k=1)  # Changed from k=10
print(f"Top match for '{query}':")
print(similar_docs[0].page_content[:500])

"""# **LOADING THE LLaMA MODEL FOR GENERATION:**"""

from transformers import LlamaTokenizer, LlamaForCausalLM
import torch

# Set model name & Hugging Face token
model_name = "meta-llama/Llama-2-7b-hf"
hf_token = #USE YOUR TOKEN DELETED MINE FOR PRIVACY REASONS

# Load Tokenizer
tokenizer = LlamaTokenizer.from_pretrained(model_name, token=hf_token)

# Load Model (Without bitsandbytes, Fully on GPU)
model = LlamaForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # Use FP16 for faster performance
    low_cpu_mem_usage=True,     # Optimize CPU memory
    token=hf_token
).to("cuda")  # Manually move to GPU

# Verify Device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Test Model with a Simple Prompt
test_prompt = "Hello, what is this model?"
inputs = tokenizer(test_prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=500)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Test response: {response}")

"""# **GENERATING ANSWERS USING RAG-BASED CHATBOT:**"""

def ask_question(query):
    # Retrieve the most relevant chunk from FAISS
    similar_docs = vector_store.similarity_search(query, k=1)
    context = similar_docs[0].page_content

    # Create a prompt with the context
    prompt = f"Based on this: {context}\nQuestion: {query}\nAnswer:"

    # Generate the response with LLaMA
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=200, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract just the answer part (after "Answer:")
    answer = response.split("Answer:")[1].strip() if "Answer:" in response else response
    return answer

# Test the chatbot
query = "What is a protocol stack?"
answer = ask_question(query)
print(f"Question: {query}")
print(f"Answer: {answer}")

"""# **RAG UI BASED CHATBOT WITH ACCURACY EVALUATION USING ROUGE**"""

import gradio as gr
from rouge_score import rouge_scorer

# Correct answer for the query
correct_answer = (
    "The 7 layers of the OSI model are:\n"
    "1. Physical Layer\n"
    "2. Data Link Layer\n"
    "3. Network Layer\n"
    "4. Transport Layer\n"
    "5. Session Layer\n"
    "6. Presentation Layer\n"
    "7. Application Layer"
)

# Function to calculate accuracy using ROUGE
def calculate_accuracy(chatbot_response, correct_answer):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(correct_answer, chatbot_response)
    return scores['rouge1'].fmeasure  # Use ROUGE-1 F1 score as accuracy

# Function to ask a question and calculate accuracy
def ask_question(query):
    # Step 1: Retrieve context from the PDF using FAISS
    similar_docs = vector_store.similarity_search(query, k=1)
    context = similar_docs[0].page_content
    print(f"Retrieved context: {context[:500]}")  # Debug: Confirm FAISS retrieval

    # Step 2: Generate response using the LLM
    prompt = (
        f"Using only this context from DSUnit1.pdf: '{context}'\n"
        f"Answer the question '{query}' by listing all steps in order, numbered 1 to N, based solely on the PDF content. "
        f"Include every step described in the context without adding details not present. "
        f"If the context lacks info for a complete answer, say 'Not enough info in the PDF to fully describe the process.'\n"
        f"Response:"
    )
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_length=700,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.3  # Reduce creativity
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = response.split("Response:")[1].strip() if "Response:" in response else response

    # Step 3: Calculate accuracy using ROUGE
    accuracy = calculate_accuracy(answer, correct_answer)

    # Return both the chatbot's response and the accuracy
    return answer, f"Accuracy (ROUGE-1 F1 Score): {accuracy:.2f}"

# Gradio interface
interface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(label="INSERT YOUR QUERY: "),
    outputs=[
        gr.Textbox(label="Answer"),
        gr.Textbox(label="Accuracy")
    ],
    title="RAG Chatbot by Harshawardhan Chitnis",
    description="Ask questions about DSUnit1.pdf, and I'll answer with detailed steps from the content!"
)
interface.launch(share=True)

"""# **CONCLUSION AND SUMMARY OF FINDINGS:**
## **In this project, we developed a Retrieval-Augmented Generation (RAG) chatbot capable of answering questions based on information extracted from a PDF document. The chatbot utilized FAISS for similarity search and an LLM (DeepSeek/LLaMA-2-7B) for response generation, ensuring responses were contextually relevant.**

## **To assess the chatbotâ€™s accuracy, we implemented ROUGE-1 F1 Score as an evaluation metric. The results demonstrated that the chatbot effectively retrieved relevant information and generated structured answers. However, accuracy varied depending on PDF content quality, retrieval precision, and LLM output coherence.**

# **CLOSING REMARKS:**
## **This project provided hands-on experience with retrieval-based NLP, combining vector search (FAISS) and LLM-based text generation. Additionally, it emphasized the importance of evaluating chatbot responses using NLP metrics like ROUGE to measure real-world effectiveness.**

## **Through this project, we gained valuable insights into:**
## **âœ… Text-based information retrieval using FAISS.**
## **âœ… Generating structured responses with LLMs.**
## **âœ… Evaluating chatbot accuracy to ensure high-quality answers.**

## **ðŸš€ This project highlights the potential of RAG-based chatbots in document-driven Q&A systems, making it a powerful tool for handling domain-specific knowledge extraction!**
"""